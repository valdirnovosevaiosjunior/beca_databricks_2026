# ğŸš€ Desafio: Pipeline de Dados de Vendas

![Databricks](https://img.shields.io/badge/Databricks-Data%20Engineering-orange?logo=databricks)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Incremental%20Load-blue?logo=delta)
![Spark](https://img.shields.io/badge/Spark-PySpark-red?logo=apache-spark)
![Status](https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow)

## ğŸ“‹ DescriÃ§Ã£o

Este repositÃ³rio contÃ©m a soluÃ§Ã£o para o desafio de construÃ§Ã£o de um pipeline de ingestÃ£o e processamento incremental de dados de vendas, utilizando arquitetura em camadas (Bronze, Silver, Gold) com Databricks, Delta Lake e PySpark.

---

## ğŸ—ï¸ Estrutura do Projeto

```
â”œâ”€â”€ cria_objetos.py           # CriaÃ§Ã£o de schema e volumes Delta
â”œâ”€â”€ destroi_objetos.py        # RemoÃ§Ã£o de schema e volumes
â”œâ”€â”€ pre_requisito.py          # VariÃ¡veis globais e configuraÃ§Ãµes
â”œâ”€â”€ desafio/
â”‚   â”œâ”€â”€ 10_desafio.py         # Notebook principal do desafio
â”‚   â””â”€â”€ dados_vendas/        # Arquivos CSV de vendas (100 arquivos)
â”œâ”€â”€ exemplos/                # Exemplos de cargas e operaÃ§Ãµes Delta
â”œâ”€â”€ exercÃ­cios/              # ExercÃ­cios prÃ¡ticos
â”œâ”€â”€ tipos_cargas/            # Scripts de geraÃ§Ã£o de massa de dados
â””â”€â”€ README.md                # Este arquivo
```

---

## ğŸ¯ Objetivo

Construir um pipeline de dados de vendas, seguindo boas prÃ¡ticas de arquitetura em camadas e mÃ©todos eficientes de carga incremental, com as seguintes etapas:

- **Bronze:** IngestÃ£o incremental dos arquivos de vendas, adicionando colunas de data de carga e nome do arquivo.
- **Silver:** TransformaÃ§Ã£o incremental usando MERGE, com atualizaÃ§Ã£o/inserÃ§Ã£o eficiente.
- **Gold:** Modelagem analÃ­tica com tabelas fato e agregada, ambas alimentadas incrementalmente.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- ![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
- ![PySpark](https://img.shields.io/badge/PySpark-Data%20Processing-orange?logo=apache-spark)
- ![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Storage-blue?logo=delta)
- ![Databricks](https://img.shields.io/badge/Databricks-Workspace-orange?logo=databricks)
- ![Git](https://img.shields.io/badge/Git-Versionamento-critical?logo=git)

---

## ğŸ§© Camadas do Pipeline

### ğŸ¥‰ Bronze
- IngestÃ£o incremental dos arquivos CSV.
- AdiÃ§Ã£o de colunas: `data_carga`, `nome_arquivo`.
- Processamento apenas de arquivos novos (idempotÃªncia).

### ğŸ¥ˆ Silver
- TransformaÃ§Ã£o e limpeza dos dados.
- Carga incremental via MERGE.
- Tabelas particionadas ou com liquid clustering.

### ğŸ¥‡ Gold
- Modelagem analÃ­tica:
	- **Fato de Vendas:** Detalhamento completo.
	- **Tabela Agregada:** Exemplo: total de vendas por perÃ­odo/produto.
- AtualizaÃ§Ã£o incremental.

---

## âš™ï¸ Boas PrÃ¡ticas e Requisitos

- ğŸš« **Sem full load:** Apenas dados novos a cada execuÃ§Ã£o.
- ğŸ“ **DocumentaÃ§Ã£o e comentÃ¡rios** em todos os notebooks.
- ğŸ—‚ï¸ **Notebooks separados** para cada camada (Bronze, Silver, Gold).
- ğŸ”„ **IdempotÃªncia:** MÃºltiplas execuÃ§Ãµes nÃ£o causam duplicidade.
- ğŸ›¡ï¸ **Tratamento de erros** e logs simples para monitoramento.
- ğŸ§ª **ValidaÃ§Ã£o de dados** em cada etapa (nulos, formatos, etc).
- ğŸ”‘ **VariÃ¡veis de ambiente** para caminhos e configs.
- ğŸ—ƒï¸ **Versionamento** via Git e integraÃ§Ã£o com Databricks Repos.
- ğŸ§© **OrquestraÃ§Ã£o** com Databricks Workflows.

---

## ğŸ“š Exemplos e ExercÃ­cios

- Exemplos prÃ¡ticos de:
	- Carga incremental (overwrite, dynamic, replaceWhere, merge)
	- Schema evolution
	- Liquid clustering
	- Change Data Feed (CDF)
- ExercÃ­cios para fixaÃ§Ã£o dos conceitos.

---

## ğŸš¦ Como Executar

1. Clone o repositÃ³rio:
	 ```bash
	 git clone https://github.com/seu-usuario/seu-repo.git
	 ```
2. Importe os notebooks para o Databricks.
3. Execute `cria_objetos.py` para criar schema e volumes.
4. Siga a ordem: Bronze â†’ Silver â†’ Gold.
5. Utilize o Databricks Workflows para orquestraÃ§Ã£o.

---